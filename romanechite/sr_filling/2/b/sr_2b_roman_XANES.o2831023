[PROLOGUE] Starting prologue on node node0746 at date Tue Oct  2 09:46:15 CEST 2018
/home/acad/ucl-naps/jank/tools/python/lib/python2.7/site-packages/pymatgen-2017.11.30-py2.7-linux-x86_64.egg/pymatgen/__init__.py:87: UserWarning: 
Pymatgen will drop Py2k support from v2019.1.1. Pls consult the documentation
at https://www.pymatgen.org for more details.
  at https://www.pymatgen.org for more details.""")
Atom indices in CIF file to compute:  ['0', '2', '3', '4', '5', '8', '9', '12', '13', '16', '17', '20', '21', '24', '25', '28', '29']
Species are:  [Element Sr, Element Ir, Element Ir, Element Ir, Element Ir, Element Ir, Element Ir, Element O, Element O, Element O, Element O, Element O, Element O, Element O, Element O, Element O, Element O]
There are residues, killing them...
Working on absorber O (atom# 12) - Edge: K
2018-10-02 09:46:20,761: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 13) - Edge: K
2018-10-02 09:50:06,462: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 16) - Edge: K
2018-10-02 09:53:57,203: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 17) - Edge: K
2018-10-02 09:58:03,210: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 20) - Edge: K
2018-10-02 10:01:58,915: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 21) - Edge: K
2018-10-02 10:05:50,421: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 24) - Edge: K
2018-10-02 10:10:16,209: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 25) - Edge: K
2018-10-02 10:14:11,837: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 28) - Edge: K
2018-10-02 10:18:02,619: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels


Working on absorber O (atom# 29) - Edge: K
2018-10-02 10:22:08,302: WARNING: pymatgen.io.feff.sets: Large system(>=14 atoms) or EXAFS calculation,                                 removing K-space settings
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/rdinp
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/atomic
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/dmdw
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/pot
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ldos
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/screen
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/opconsat
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/xsph
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/fms
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/mkgtr
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/path
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/genfmt
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/ff2x
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/sfconv
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/compton
  exec: mpirun /home/acad/ucl-naps/jank/JFEFF/feff90/unix/MPI/eels



------------------------------- PBS server and MOM logs ------------------------------- 


------------------frontal2.cenaero.be------------------

Job: 2831023.frontal2

10/02/2018 06:44:20  S    enqueuing into main, state 1 hop 1
10/02/2018 06:44:20  S    dequeuing from main, state 1
10/02/2018 06:44:20  S    enqueuing into main_has, state 1 hop 1
10/02/2018 06:44:20  S    Job Queued at request of jank@frontal3, owner = jank@frontal3, job name = sr_2b_roman_XANES, queue = main_has
10/02/2018 06:44:20  A    queue=main
10/02/2018 06:44:20  A    queue=main_has
10/02/2018 09:26:54  L    Job is a top job and will run at Tue Oct  2 15:24:47 2018
10/02/2018 09:27:25  L    Job is a top job and will run at Tue Oct  2 15:24:47 2018
10/02/2018 09:28:30  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:28:59  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:29:29  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:30:07  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:30:39  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:31:10  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:31:40  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:32:10  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:32:40  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:33:12  L    Job is a top job and will run at Tue Oct  2 15:03:36 2018
10/02/2018 09:33:52  L    Job is a top job and will run at Tue Oct  2 14:37:54 2018
10/02/2018 09:34:22  L    Job is a top job and will run at Tue Oct  2 14:25:44 2018
10/02/2018 09:34:52  L    Job is a top job and will run at Tue Oct  2 14:25:44 2018
10/02/2018 09:35:24  L    Job is a top job and will run at Tue Oct  2 14:25:44 2018
10/02/2018 09:35:56  L    Job is a top job and will run at Tue Oct  2 14:25:44 2018
10/02/2018 09:36:27  L    Job is a top job and will run at Tue Oct  2 14:08:06 2018
10/02/2018 09:36:59  L    Job is a top job and will run at Tue Oct  2 14:08:06 2018
10/02/2018 09:37:31  L    Job is a top job and will run at Tue Oct  2 14:08:06 2018
10/02/2018 09:38:03  L    Job is a top job and will run at Tue Oct  2 14:08:06 2018
10/02/2018 09:38:35  L    Job is a top job and will run at Tue Oct  2 14:08:06 2018
10/02/2018 09:39:08  L    Job is a top job and will run at Tue Oct  2 14:08:06 2018
10/02/2018 09:39:38  L    Job is a top job and will run at Tue Oct  2 13:57:17 2018
10/02/2018 09:40:09  L    Job is a top job and will run at Tue Oct  2 13:57:17 2018
10/02/2018 09:40:40  L    Job is a top job and will run at Tue Oct  2 13:57:17 2018
10/02/2018 09:41:05  L    Job is a top job and will run at Tue Oct  2 13:57:17 2018
10/02/2018 09:41:30  L    Job is a top job and will run at Tue Oct  2 13:57:17 2018
10/02/2018 09:41:54  L    Job is a top job and will run at Tue Oct  2 13:36:33 2018
10/02/2018 09:42:20  L    Job is a top job and will run at Tue Oct  2 13:36:33 2018
10/02/2018 09:42:48  L    Job is a top job and will run at Tue Oct  2 13:36:33 2018
10/02/2018 09:43:13  L    Job is a top job and will run at Tue Oct  2 13:26:55 2018
10/02/2018 09:43:38  L    Job is a top job and will run at Tue Oct  2 13:26:55 2018
10/02/2018 09:44:05  L    Job is a top job and will run at Tue Oct  2 13:26:55 2018
10/02/2018 09:44:31  L    Job is a top job and will run at Tue Oct  2 12:42:25 2018
10/02/2018 09:44:57  L    Job is a top job and will run at Tue Oct  2 12:42:25 2018
10/02/2018 09:45:22  L    Job is a top job and will run at Tue Oct  2 12:42:25 2018
10/02/2018 09:45:49  L    Job is a top job and will run at Tue Oct  2 12:42:25 2018
10/02/2018 09:45:49  L    Queue main_has per-user limit reached on resource ncpus
10/02/2018 09:46:15  L    Considering job to run
10/02/2018 09:46:15  L    Job run
10/02/2018 09:46:15  A    user=jank group=catalys project=generic jobname=sr_2b_roman_XANES queue=main_has ctime=1538455460 qtime=1538455460 etime=1538455460 start=1538466375 exec_host=node0746/0*12 exec_vnode=(node0746:ncpus=12:mem=25600000kb) Resource_List.mem=25000mb Resource_List.mem_pnode=63000mb Resource_List.model=haswell_fit Resource_List.mpiprocs=12 Resource_List.ncpus=12 Resource_List.ncpus_pnode=24 Resource_List.nodect=1 Resource_List.place=free Resource_List.rncpus=12 Resource_List.select=1:ncpus=12:mem=25000mb:mpiprocs=12:ompthreads=1 Resource_List.walltime=06:00:00 resource_assigned.mem=25600000kb resource_assigned.ncpus=12

------------------node0746------------------

Job: 2831023.frontal2

10/02/2018 09:46:15  M    running prologue
10/02/2018 09:46:15  M    Started, pid = 17093
10/02/2018 10:26:09  M    task 00000001 terminated
10/02/2018 10:26:09  M    Terminated
10/02/2018 10:26:09  M    task 00000001 cput= 5:00:30
10/02/2018 10:26:09  M    node0746 cput= 4:47:53 mem=2171904kb
10/02/2018 10:26:09  M    update_job_usage: CPU usage: 18033.867 secs
10/02/2018 10:26:09  M    update_job_usage: Memory usage: mem=2171904kb
10/02/2018 10:26:09  M    update_job_usage: Memory usage: vmem=2171904kb
10/02/2018 10:26:09  M    no active tasks
10/02/2018 10:26:09  M    copy file request received
10/02/2018 10:26:09  M    staged 1 items out over 0:00:00
10/02/2018 10:26:09  M    no active tasks
10/02/2018 10:26:09  M    delete job request received


------------------------------- Job Information ------------------------------- 

Job Owner       : jank@frontal3 
Job Project     : catalys 
Job Name        : sr_2b_roman_XANES 
Job Id          : 2831023.frontal2 
Job Queue       : main_has 
Job Exit Status : 0 

Resources Requested 

Number of Cores per Job                - NCPUS_PJOB  : 12
Total Memory per Job                   - MEM_PJOB    : 25000mb
Placement                                            : free
Execution Time                         - WALLTIME    : 06:00:00

Resources Used 

Total Memory used                      - MEM              : 2171904kb
Total CPU Time                         - CPU_Time         : 05:00:34
Execution Time                         - Wall_Time        : 00:39:55
Ncpus x Execution Time                 - N_Wall_Time      : 7:59:00
CPU_Time / N_Wall_Time (%)             - ETA              : 62%
Number of Mobilized Resources per Job  - NCPUS_EQUIV_PJOB : 12
Mobilized Resources x Execution Time   - R_Wall_Time      : 7:59:00
CPU_Time / R_Wall_Time (%)             - ALPHA            : 62%

For metrics definition, please refer to https://tier1.cenaero.be/en/faq-page 

------------------------------------------------------------------------------- 

